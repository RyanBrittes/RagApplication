# ü§ñ Retrieval-Augmented Generation :arrows_clockwise:
Aplica√ß√£o que utiliza o RAG para tornar as respostas da LLM mais precisas com base em um contexto

[![Status](https://img.shields.io/badge/status-Em%20Desenvolvimento-yellow)]()
[![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)]()
[![Docker](https://img.shields.io/badge/Docker-Container-blue?logo=docker)]()
[![Ollama](https://img.shields.io/badge/LLM-Ollama-lightgrey?logo=rocket)]()
[![RAG](https://img.shields.io/badge/IA-RAG-green?logo=github)]()
[![Feito por Ryan](https://img.shields.io/badge/feito%20por-Ryan%20Brittes-blue)]()
---

## :beginner: Tecnologias utilizadas:
- **Python**
- **Ollama**
- **Docker**
- **Chroma**
- **nomic-embed-text**

## :pencil: Explica√ß√£o
Com este reposit√≥rio voc√™ poder√° entender um pouco melhor de como a t√©cnica RAG funciona e como pode ser ben√©fica para seus projetos de IA. Se j√° tem alguma no√ß√£o de como o RAG funciona e deseja ir direto ao como o c√≥digo funciona, sugiro que pule para pr√≥xim estapa.

### ***RAG - RETRIEVAL-AUGMENTED GENERATION:***
> √â uma tecnica utilizada para melhorar a precis√£o das respostas das LLMs, diminuindo assim alucina√ß√µes e imprecis√µes.

1. Por meio do **Input** de entrada do usu√°rio, transformamos esse input do tipo String em **Vetores** para que haja maior efici√™ncia na busca de informa√ß√µes semelhantes.
2. Ap√≥s fazemos uma consulta com um **VectorSearch** em um banco de dados vetorial que est√° com informa√ß√µes pr√©-armazenadas de um **Document-Retrieval** (cont√©m informa√ß√µes sobre algum contexto espec√≠fico que possa desejar que a LLM saiba para responder com maior precis√£o, como por exemplo: informa√ß√µes pessoais, dados que n√£o estavam contidos no treinamento ou informa√ß√µes muito espec√≠ficas), esse documento foi separado em **Chunks** e cada um de seus Chunks foi transformado em vetores, assim conseguimos particionar a busca pelas informa√ß√µes e tornamos a pesquisa algo mais r√°pido e preciso.
3. Depois que da busca no banco de dados s√£o retornados valores que s√£o semelhantes ao input do usu√°rio, assim comparamos os vetores semelhantes, como sabemos quais s√£o os vetores associados √†s palavras separadas em Chunks, podemos recuperar essas informa√ß√µes e repassar os resultados semelhantes para a LLM por meio de um **Prompt** com o contexto contido.
4. Ent√£o criamos um prompt e enviamos este para a LLM formular um **Output** ao usu√°rio.
### ***FUNCIONAMENTO:***
Com base no seguinte fluxo entenderemos melhor como o processo do RAG funciona:
```mermaid
graph TD
    A[Input do Usu√°rio] --> B[RAG]
    B --> C[VectorStore]
    C --> D[Chunks]
    C --> E[Embeddings]
    E --> D[Chunks]
    D --> F[Documento PDF]
    B --> G[Prompt com contexto]
    G --> H[LLM via Ollama]
    H --> I[Output da LLM]
```
- **Input do usu√°rio:** √© realizado um input e entramos no RAG.
- **RAG:** no RAG entramos no VectorStore e ent√£o no Embeddings para transformar a pergunta em Vetores.
- **VectorStore:* utiliza um VectorSearch para pesquisar dentro das informa√ß√µes armazenadas quais s√£o os vetores mais semelhantes aos do Input.
- **Embeddings:** tem a fun√ß√£o de transformar os Chunks em vetores e armazenar estar informa√ß√µes no banco de dados
- **Chunks:** tem a fun√ß√£o de separar o Documento PDF em Chunks, ou peda√ßos menores.
- Prompt com contexto: ap√≥s terem sido retornado vetores associados ao input criamos um prompt com o contexto das informa√ß√µes e repassamos para a LLM.
- **LLM via Ollama:** a LLM recebe o prompt e com base nele ela responde ao Input inicial do Usu√°rio
- **Output da LLM:** resposta totalmente baseada no Document-Retrieval, ofercendo maior precis√£o ao contexto do usu√°rio.
- 
## :rocket: Explica√ß√£o do C√≥digo:
Com o fluxo do c√≥digo abaixo:
```
RagApp
  ‚îî‚îÄ‚îÄApp
  ‚îÇ   ‚îú‚îÄ‚îÄchunks.py
  ‚îÇ   ‚îú‚îÄ‚îÄembed.py
  ‚îÇ   ‚îú‚îÄ‚îÄextractor.py
  ‚îÇ   ‚îú‚îÄ‚îÄollamaLLM.py
  ‚îÇ   ‚îú‚îÄ‚îÄragApp.py
  ‚îÇ   ‚îî‚îÄ‚îÄvectorStore.py
  ‚îî‚îÄ‚îÄChromaDB
  ‚îî‚îÄ‚îÄFiles
  ‚îî‚îÄ‚îÄ.env
  ‚îî‚îÄ‚îÄdocker-compose.yml
  ‚îî‚îÄ‚îÄrequirements.txt
```
### App:
- chunks.py: Respons√°vel por separar o documento de texto extraido do PDF pela fun√ß√£o ***extractor.py*** em chunks, ou seja, peda√ßos menores, com o m√©todo ***split_data***. Foi configurado um Chunk Size de 100 caracteres e Overlap de 30 (sobreposi√ß√£o que garante que o contexto de um chunk n√£o seja afetado)
- embed.py: Utilizando o criador de embeds ***nomic***, iremos recuperar o texto separado em chunks de ***chunks.py*** e transformaremos cada um destes chunks em vetores  com o m√©todo ***embed_text***. O m√©todo ***embed_query*** √© acionado quando queremos transformar o input em vetores. OBS: Este c√≥digo est√° processando os embbedings a partir de uma API pelo Atlas NOMIC AI (Encontrar na documenta√ß√£o), por√©m no c√≥digo tem a implementa√ß√£o local para caso queira usar a placa de v√≠deo ou cpu para processar os embeddings.
- extractor.py: Extrai o texto do PDF armazenado na pasta ***Files*** em texto com o m√©todo ***extract_text***.
- ollamaLLM.py: Em um loop de conversa com a LLM, fornecemos o ***Input*** inicial e a ***Persona*** que a LLM dever√° seguir, utilizamos o m√©todo ***compair_vector*** armazenado em ***ragApp.py*** para iniciar o fluxo de cria√ß√£o do prompt para a LLM. Aqui voc√™ encontrar√° os par√¢metros informados para que a comunica√ß√£o da LLM rodando localmente funcione, nesse exemplo utilizamos o modelo do ollama ***gemma3:1b***.
- ragApp.py: Aqui s√£o realizados os processos de pegar o input fornecido pelo usu√°rio e ent√£o transform√°-lo em vetor e retornar o resultado que o ***VectorSearch*** encontrou de vetores mais pr√≥ximos para formular o prompt, utilizando o m√©todo ***compair_vector***
- vectorStore.py: Gerencamento do banco de dados vetorial com os m√©todos de ***verifica√ß√£o/cria√ß√£o (collection_verify_create)***, ***adi√ß√£o de dados ao banco (collection_add)*** e ***consulta com o VectorSearch trazendo os 3 resultados mais semelhantes (collection_query)***.
### ChromaDB:
- Arquivo necess√°rio para que o ***vectorStore.py*** saiba onde armazenar o banco de dados, local onde ser√£o armazenados os dados que ser√£o criados com base no documento fornecido.
### Files:
- Arquivo necess√°rio para que o ***extractor.py*** saiba de onde pegar o PDF e transcrever as informa√ß√µes, nele deve ser armazenado o PDF que ser√° utilizado no RAG.
### .env:
- Arquivo com informa√ß√µes necess√°rio para que o container docker do ollama funcione.
### docker-compose.yml:
- Arquivo que cont√©m o container configurado no modelo de LLM do ollama ***gemma3:1b(modelo utilizado no c√≥digo)***, por√©m este pode ser alterado √† prefer√™ncia do usu√°rio.
### requirements.txt:
- Documento com todas as bibliotecas utilizadas no c√≥digo, para adicion√°-las com facilidade d√™ o comando ***(pip install -r requirements.txt)***.

## :computer: Implementa√ß√£o Pr√°tica:
Para implementar o c√≥digo e realizar seus testes localmente, clone o reposit√≥rio em um pasta com:
```
git clone https://github.com/RyanBrittes/RagApplication.git
```
Inicialize o ollama no container indo √† pasta clonada do reposit√≥rio pelo bash e dando o comando:
```
docker compose up -d
ou
docker-compose up -d
(dependendo da sua vers√£o do docker)
```
Com o ollama funcionando, entre na pasta App do c√≥digo e d√™ start no arquivo ***ollamaLLM.py***, se estiver no bash:
```
python ollamaLLM.py
```
## Documenta√ß√£o adicional:
Caso queira encontrar uma documenta√ß√£o adicional das tecnologias utilizadas, seguem os arquivos:
| Tecnologia | Doc   |
|---------------|----------------|
| Ollama   | [gemma3:1b](https://ollama.com/library/gemma3)   |
| Chroma | [Reposit√≥rio](https://github.com/chroma-core/chroma?tab=readme-ov-file)    |
| Chroma | [Site](https://docs.trychroma.com/docs/overview/introduction) |
| Nomic  |  [Reposit√≥rio](https://github.com/nomic-ai)  |
| Nomic  |  [Site](https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post)  |
| Nomic  |  [Site](https://docs.nomic.ai/atlas/embeddings-and-retrieval/text-embedding)  |

## Considera√ß√µes finais:
Estou dispon√≠vel para caso hajam d√∫vidas ou dicas de melhorias, abaixo encontre os meios de contato comigo:
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-blue?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ryanbrittes/)
